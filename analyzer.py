"""
TikToké£é™©åˆ†æå™¨ - å¤šè¯­è¨€æ¨¡å‹ç‰ˆæœ¬
ä½¿ç”¨æœ¬åœ°é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå†…å®¹é£é™©åˆ†æ
"""

import re
import json
import time
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional


class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ - åŠ è½½å’Œç®¡ç†æ‰€æœ‰éœ€è¦çš„æ¨¡å‹"""

    def __init__(self):
        self.models = {}
        self._init_models()

    def _init_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰éœ€è¦çš„æ¨¡å‹"""
        print("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")

        try:
            # 1. æƒ…æ„Ÿåˆ†ææ¨¡å‹ (æ”¯æŒå¤šè¯­è¨€)
            print("åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹...")
            from transformers import pipeline

            self.models["sentiment"] = pipeline(
                "sentiment-analysis",
                model="cardiffnlp/twitter-xlm-roberta-base-sentiment",
                device=-1,  # ä½¿ç”¨CPU
                max_length=512,
                truncation=True,
            )
            print("âœ… æƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½æˆåŠŸ")

            # 2. æ¯’æ€§æ£€æµ‹æ¨¡å‹ (è‹±è¯­)
            print("åŠ è½½æ¯’æ€§æ£€æµ‹æ¨¡å‹...")
            try:
                self.models["toxicity"] = pipeline(
                    "text-classification",
                    model="unitary/toxic-bert",
                    device=-1,
                    max_length=512,
                    truncation=True,
                )
                print("âœ… æ¯’æ€§æ£€æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ")
            except:
                # å¦‚æœåŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ¨¡å‹
                self.models["toxicity"] = pipeline(
                    "text-classification",
                    model="distilbert-base-uncased-finetuned-sst-2-english",
                    device=-1,
                )
                print("âœ… ä½¿ç”¨å¤‡ç”¨æƒ…æ„Ÿæ¨¡å‹è¿›è¡Œæ¯’æ€§æ£€æµ‹")

            # 3. ä»‡æ¨è¨€è®ºæ£€æµ‹ (å¤šè¯­è¨€)
            print("åŠ è½½ä»‡æ¨è¨€è®ºæ£€æµ‹æ¨¡å‹...")
            try:
                self.models["hate"] = pipeline(
                    "text-classification",
                    model="Hate-speech-CNERG/dehatebert-mono-english",
                    device=-1,
                    max_length=512,
                    truncation=True,
                )
                print("âœ… ä»‡æ¨è¨€è®ºæ£€æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ")
            except:
                print("âš ï¸ ä»‡æ¨è¨€è®ºæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨è§„åˆ™æ£€æµ‹")
                self.models["hate"] = None

            print("ğŸ‰ æ‰€æœ‰æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            print("âš ï¸ å°†ä½¿ç”¨è½»é‡çº§æ¨¡å¼è¿è¡Œ")
            self.models = {}
            raise e


# ============================== è¯­è¨€æ£€æµ‹ ==============================


class LanguageDetector:
    """è¯­è¨€æ£€æµ‹å™¨"""

    def __init__(self):
        try:
            from langdetect import detect, DetectorFactory

            DetectorFactory.seed = 0
            self.detect_func = detect
            self.has_langdetect = True
        except ImportError:
            print("âš ï¸ langdetectåº“æœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•è¯­è¨€æ£€æµ‹")
            self.has_langdetect = False
            self._init_simple_detector()

    def _init_simple_detector(self):
        """åˆå§‹åŒ–ç®€å•è¯­è¨€æ£€æµ‹å™¨"""
        self.language_patterns = {
            "zh": re.compile(r"[\u4e00-\u9fff]"),  # ä¸­æ–‡
            "ja": re.compile(r"[\u3040-\u309f\u30a0-\u30ff]"),  # æ—¥æ–‡
            "ko": re.compile(r"[\uac00-\ud7af]"),  # éŸ©æ–‡
            "ar": re.compile(r"[\u0600-\u06ff]"),  # é˜¿æ‹‰ä¼¯æ–‡
            "ru": re.compile(r"[\u0400-\u04ff]"),  # ä¿„æ–‡
        }

        self.common_words = {
            "en": {"the", "and", "you", "that", "have", "for", "with"},
            "zh": {"çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ"},
            "fr": {"le", "la", "et", "les", "des", "est", "pas"},
            "de": {"der", "die", "das", "und", "ist", "nicht"},
            "es": {"el", "la", "y", "en", "que", "los", "las"},
            "ja": {"ã®", "ã«", "ã¯", "ã‚’", "ãŸ", "ã§", "ãŒ"},
            "ko": {"ì´", "ê°€", "ì„", "ë¥¼", "ì€", "ëŠ”", "ì—"},
        }

    def detect(self, text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
        if not text or len(text.strip()) < 10:
            return "en"  # æ–‡æœ¬å¤ªçŸ­ï¼Œé»˜è®¤è‹±è¯­

        if self.has_langdetect:
            try:
                return self.detect_func(text)
            except:
                return self._simple_detect(text)
        else:
            return self._simple_detect(text)

    def _simple_detect(self, text: str) -> str:
        """ç®€å•è¯­è¨€æ£€æµ‹"""
        text_lower = text.lower()

        # æ£€æŸ¥å­—ç¬¦èŒƒå›´
        for lang, pattern in self.language_patterns.items():
            if pattern.search(text):
                return lang

        # æ£€æŸ¥å¸¸è§è¯æ±‡
        words = re.findall(r"\b\w+\b", text_lower)
        lang_scores = {}

        for lang, common_words in self.common_words.items():
            score = sum(1 for word in words if word in common_words)
            if score > 0:
                lang_scores[lang] = score

        if lang_scores:
            return max(lang_scores.items(), key=lambda x: x[1])[0]

        # é»˜è®¤è‹±è¯­
        return "en"


# ============================== æ¨¡å‹åˆ†æå™¨ ==============================


class ModelAnalyzer:
    """ä½¿ç”¨æ¨¡å‹è¿›è¡Œåˆ†æ"""

    def __init__(self, model_manager: ModelManager):
        self.models = model_manager.models
        self.language_detector = LanguageDetector()

        # å¤‡ç”¨å…³é”®è¯æ•°æ®åº“
        self._init_backup_keywords()

    def _init_backup_keywords(self):
        """åˆå§‹åŒ–å¤‡ç”¨å…³é”®è¯åº“"""
        self.toxic_keywords = {
            "en": {"idiot", "stupid", "moron", "dumb", "retard", "fool", "loser"},
            "zh": {"ç™½ç—´", "ç¬¨è›‹", "è ¢è´§", "å‚»ç“œ", "åºŸç‰©", "åƒåœ¾"},
            "ja": {"ãƒã‚«", "ã‚¢ãƒ›", "é¦¬é¹¿", "é–“æŠœã‘"},
            "ko": {"ë°”ë³´", "ë©ì²­ì´", "ë“±ì‹ ", "ë¯¸ì¹œë†ˆ"},
            "fr": {"idiot", "stupide", "imbÃ©cile", "crÃ©tin"},
            "de": {"Idiot", "Dummkopf", "Trottel", "Arschloch"},
            "es": {"idiota", "estÃºpido", "imbÃ©cil", "cretino"},
        }

        self.hate_keywords = {
            "en": {"hate", "kill", "destroy", "attack", "murder", "exterminate"},
            "zh": {"æ¨", "æ€", "æ­»", "æ¶ˆç­", "ç ´å"},
            "ja": {"æ†ã‚€", "æ®ºã™", "æ­»ã­", "æ¶ˆãˆã‚"},
            "ko": {"ì¦ì˜¤", "ì£½ì—¬", "ì£½ì–´", "ì—†ì• "},
            "fr": {"haine", "tuer", "dÃ©truire", "attaquer"},
            "de": {"hassen", "tÃ¶ten", "zerstÃ¶ren", "angreifen"},
            "es": {"odiar", "matar", "destruir", "atacar"},
        }

    def analyze_with_models(
        self, text: str, language: str = "auto"
    ) -> Dict[str, float]:
        """ä½¿ç”¨æ¨¡å‹åˆ†ææ–‡æœ¬"""
        if not self.models:
            return self._analyze_with_keywords(text, language)

        try:
            # æ£€æµ‹è¯­è¨€
            if language == "auto":
                detected_lang = self.language_detector.detect(text)
            else:
                detected_lang = language

            results = {}

            # 1. ä½¿ç”¨æƒ…æ„Ÿåˆ†ææ¨¡å‹
            try:
                sentiment_result = self.models["sentiment"](text[:512])[0]
                if isinstance(sentiment_result, list):
                    sentiment_result = sentiment_result[0]

                label = sentiment_result["label"].lower()
                score = sentiment_result["score"]

                # è´Ÿé¢æƒ…æ„Ÿå¼ºåº¦
                if "negative" in label or "neg" in label:
                    emotional_intensity = min(score * 1.2, 1.0)
                elif "positive" in label or "pos" in label:
                    emotional_intensity = score * 0.3  # æ­£é¢æƒ…æ„Ÿå¼ºåº¦è¾ƒä½
                else:
                    emotional_intensity = score * 0.5

                results["emotional_intensity"] = emotional_intensity
            except Exception as e:
                print(f"æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
                results["emotional_intensity"] = self._estimate_emotional_intensity(
                    text
                )

            # 2. ä½¿ç”¨æ¯’æ€§æ£€æµ‹æ¨¡å‹
            try:
                if self.models.get("toxicity"):
                    toxicity_result = self.models["toxicity"](text[:512])[0]
                    if isinstance(toxicity_result, list):
                        toxicity_result = toxicity_result[0]

                    label = toxicity_result["label"].lower()
                    score = toxicity_result["score"]

                    if "toxic" in label or "negative" in label or "neg" in label:
                        toxicity = min(score * 1.1, 1.0)
                    else:
                        toxicity = score * 0.5

                    results["toxicity"] = toxicity
                else:
                    results["toxicity"] = self._estimate_toxicity(text, detected_lang)
            except Exception as e:
                print(f"æ¯’æ€§æ£€æµ‹å¤±è´¥: {e}")
                results["toxicity"] = self._estimate_toxicity(text, detected_lang)

            # 3. ä½¿ç”¨ä»‡æ¨è¨€è®ºæ£€æµ‹æ¨¡å‹
            try:
                if self.models.get("hate"):
                    hate_result = self.models["hate"](text[:512])[0]
                    if isinstance(hate_result, list):
                        hate_result = hate_result[0]

                    label = hate_result["label"].lower()
                    score = hate_result["score"]

                    if "hate" in label or "offensive" in label:
                        hate_score = min(score * 1.2, 1.0)
                    else:
                        hate_score = score * 0.3

                    results["hate_targeting"] = hate_score
                else:
                    results["hate_targeting"] = self._estimate_hate_targeting(
                        text, detected_lang
                    )
            except Exception as e:
                print(f"ä»‡æ¨æ£€æµ‹å¤±è´¥: {e}")
                results["hate_targeting"] = self._estimate_hate_targeting(
                    text, detected_lang
                )

            # 4. æ”¿æ²»ç›¸å…³æ€§åˆ†æï¼ˆåŸºäºå…³é”®è¯ï¼‰
            results["political_relevance"] = self._analyze_political_relevance(
                text, detected_lang
            )

            return results

        except Exception as e:
            print(f"æ¨¡å‹åˆ†æå¤±è´¥: {e}")
            return self._analyze_with_keywords(text, language)

    def _analyze_with_keywords(self, text: str, language: str) -> Dict[str, float]:
        """ä½¿ç”¨å…³é”®è¯åˆ†æï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        if language == "auto":
            detected_lang = self.language_detector.detect(text)
        else:
            detected_lang = language

        return {
            "toxicity": self._estimate_toxicity(text, detected_lang),
            "hate_targeting": self._estimate_hate_targeting(text, detected_lang),
            "emotional_intensity": self._estimate_emotional_intensity(text),
            "political_relevance": self._analyze_political_relevance(
                text, detected_lang
            ),
        }

    def _estimate_toxicity(self, text: str, language: str) -> float:
        """ä¼°ç®—æ¯’æ€§åˆ†æ•°"""
        text_lower = text.lower()
        score = 0.0

        # æ£€æŸ¥å…³é”®è¯
        keywords = self.toxic_keywords.get(language, self.toxic_keywords["en"])
        found_keywords = sum(1 for word in keywords if word in text_lower)

        if found_keywords > 0:
            score += min(0.6, found_keywords * 0.15)

        # æ£€æŸ¥å¤§å†™
        if len(text) > 10 and text.isupper():
            score += 0.3

        # æ£€æŸ¥æ„Ÿå¹å·
        exclamation_count = text.count("!")
        if exclamation_count > 0:
            score += min(0.2, exclamation_count * 0.05)

        return min(score, 1.0)

    def _estimate_hate_targeting(self, text: str, language: str) -> float:
        """ä¼°ç®—ä»‡æ¨ç›®æ ‡åˆ†æ•°"""
        text_lower = text.lower()
        score = 0.0

        # æ£€æŸ¥ä»‡æ¨å…³é”®è¯
        keywords = self.hate_keywords.get(language, self.hate_keywords["en"])
        found_keywords = sum(1 for word in keywords if word in text_lower)

        if found_keywords > 0:
            score += min(0.5, found_keywords * 0.2)

        # æ£€æŸ¥ç¾¤ä½“æ€§è¯­è¨€
        group_patterns = [
            r"all\s+\w+\s+are",
            r"every\s+\w+\s+is",
            r"they\s+all",
            r"those\s+\w+\s+",
        ]

        for pattern in group_patterns:
            if re.search(pattern, text_lower, re.IGNORECASE):
                score += 0.3
                break

        return min(score, 1.0)

    def _estimate_emotional_intensity(self, text: str) -> float:
        """ä¼°ç®—æƒ…ç»ªå¼ºåº¦"""
        score = 0.0

        # æ£€æŸ¥æ ‡ç‚¹
        exclamation_count = text.count("!")
        if exclamation_count >= 5:
            score += 0.4
        elif exclamation_count >= 3:
            score += 0.3
        elif exclamation_count >= 1:
            score += 0.15

        question_count = text.count("?")
        if question_count >= 3:
            score += 0.2

        # æ£€æŸ¥å¤§å†™
        if len(text) > 20:
            upper_count = sum(1 for c in text if c.isupper())
            upper_ratio = upper_count / len(text)
            if upper_ratio > 0.5:
                score += 0.3

        # æ£€æŸ¥å¼ºåº¦è¯æ±‡
        intensity_words = {"very", "extremely", "absolutely", "completely", "totally"}
        text_lower = text.lower()
        intensity_count = sum(1 for word in intensity_words if word in text_lower)
        score += min(0.3, intensity_count * 0.1)

        return min(score, 1.0)

    def _analyze_political_relevance(self, text: str, language: str) -> float:
        """åˆ†ææ”¿æ²»ç›¸å…³æ€§"""
        # æ”¿æ²»å…³é”®è¯ï¼ˆå¤šè¯­è¨€ï¼‰
        political_keywords = {
            "en": {"government", "president", "election", "vote", "policy", "law"},
            "zh": {"æ”¿åºœ", "æ€»ç»Ÿ", "é€‰ä¸¾", "æŠ•ç¥¨", "æ”¿ç­–", "æ³•å¾‹"},
            "ja": {"æ”¿åºœ", "å¤§çµ±é ˜", "é¸æŒ™", "æŠ•ç¥¨", "æ”¿ç­–", "æ³•å¾‹"},
            "ko": {"ì •ë¶€", "ëŒ€í†µë ¹", "ì„ ê±°", "íˆ¬í‘œ", "ì •ì±…", "ë²•ë¥ "},
            "fr": {"gouvernement", "prÃ©sident", "Ã©lection", "vote", "politique", "loi"},
            "de": {"Regierung", "PrÃ¤sident", "Wahl", "Stimme", "Politik", "Gesetz"},
            "es": {"gobierno", "presidente", "elecciÃ³n", "voto", "polÃ­tica", "ley"},
        }

        text_lower = text.lower()
        keywords = political_keywords.get(language, political_keywords["en"])

        found_keywords = sum(1 for word in keywords if word in text_lower)

        if found_keywords == 0:
            return 0.0
        elif found_keywords >= 3:
            return 0.7
        elif found_keywords >= 2:
            return 0.5
        else:
            return 0.3


# ============================== ä¸»åˆ†æå™¨ ==============================


class TiktokRiskAnalyzer:
    """TikToké£é™©åˆ†æå™¨ä¸»ç±»"""

    def __init__(self):
        print("=" * 60)
        print("ğŸ¬ TikTokå¤šè¯­è¨€é£é™©åˆ†æå™¨ - æ¨¡å‹ç‰ˆæœ¬")
        print("=" * 60)

        # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        self.model_manager = ModelManager()

        # åˆå§‹åŒ–åˆ†æå™¨
        self.model_analyzer = ModelAnalyzer(self.model_manager)

        # ç»´åº¦æƒé‡
        self.dimension_weights = {
            "toxicity": 0.35,
            "hate_targeting": 0.35,
            "emotional_intensity": 0.20,
            "political_relevance": 0.10,
        }

        # é£é™©ç­‰çº§é˜ˆå€¼
        self.risk_thresholds = {"LOW": 0.2, "MODERATE": 0.4, "HIGH": 0.7, "SEVERE": 0.9}

        print("âœ… åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")

    def analyze(self, text: str, language: str = "auto") -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬é£é™©"""
        start_time = time.time()

        # è¾“å…¥éªŒè¯
        if not text or not isinstance(text, str):
            return self._error_result("è¾“å…¥æ–‡æœ¬ä¸ºç©ºæˆ–æ— æ•ˆ")

        text = text.strip()
        if len(text) == 0:
            return self._error_result("è¾“å…¥æ–‡æœ¬ä¸ºç©º")

        try:
            # 1. ä½¿ç”¨æ¨¡å‹åˆ†æå„ä¸ªç»´åº¦
            dimensions = self.model_analyzer.analyze_with_models(text, language)

            # 2. è®¡ç®—ç»¼åˆé£é™©åˆ†æ•°
            risk_score = self._calculate_risk_score(dimensions)

            # 3. ç¡®å®šé£é™©ç­‰çº§
            risk_level = self._determine_risk_level(risk_score)

            # 4. ç”Ÿæˆè§£é‡Š
            explanations = self._generate_explanations(dimensions, risk_level)

            # 5. è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_confidence(text, dimensions)

            # 6. æ£€æµ‹è¯­è¨€
            detected_language = self.model_analyzer.language_detector.detect(text)

            # 7. æ„å»ºç»“æœ
            result = {
                "success": True,
                "risk_level": risk_level,
                "risk_score": round(risk_score, 3),
                "dimensions": {
                    "toxicity": round(dimensions.get("toxicity", 0), 3),
                    "hate_targeting": round(dimensions.get("hate_targeting", 0), 3),
                    "emotional_intensity": round(
                        dimensions.get("emotional_intensity", 0), 3
                    ),
                    "political_relevance": round(
                        dimensions.get("political_relevance", 0), 3
                    ),
                },
                "explanations": explanations,
                "confidence": round(confidence, 2),
                "detected_language": detected_language,
                "original_language": language,
                "processing_time": round(time.time() - start_time, 3),
                "timestamp": datetime.now().isoformat(),
            }

            return result

        except Exception as e:
            print(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            return self._error_result(f"åˆ†æå¤±è´¥: {str(e)}")

    def _calculate_risk_score(self, dimensions: Dict[str, float]) -> float:
        """è®¡ç®—ç»¼åˆé£é™©åˆ†æ•°"""
        total = 0.0
        for key, weight in self.dimension_weights.items():
            total += dimensions.get(key, 0) * weight
        return min(total, 1.0)

    def _determine_risk_level(self, score: float) -> str:
        """ç¡®å®šé£é™©ç­‰çº§"""
        if score >= self.risk_thresholds["SEVERE"]:
            return "SEVERE"
        elif score >= self.risk_thresholds["HIGH"]:
            return "HIGH"
        elif score >= self.risk_thresholds["MODERATE"]:
            return "MODERATE"
        else:
            return "LOW"

    def _generate_explanations(
        self, dimensions: Dict[str, float], risk_level: str
    ) -> List[str]:
        """ç”Ÿæˆè§£é‡Šè¯´æ˜"""
        explanations = []

        if dimensions.get("toxicity", 0) > 0.6:
            explanations.append("æ£€æµ‹åˆ°ä¾®è¾±æ€§æˆ–æ”»å‡»æ€§è¯­è¨€")
        elif dimensions.get("toxicity", 0) > 0.3:
            explanations.append("åŒ…å«è½»å¾®ä¸å½“ç”¨è¯­")

        if dimensions.get("hate_targeting", 0) > 0.6:
            explanations.append("å­˜åœ¨ä»‡æ¨è¨€è®ºæˆ–ç¾¤ä½“é’ˆå¯¹æ€§å†…å®¹")
        elif dimensions.get("hate_targeting", 0) > 0.3:
            explanations.append("æ¶‰åŠç¾¤ä½“è´Ÿé¢è¡¨è¾¾")

        if dimensions.get("emotional_intensity", 0) > 0.6:
            explanations.append("æƒ…ç»ªè¡¨è¾¾éå¸¸å¼ºçƒˆ")
        elif dimensions.get("emotional_intensity", 0) > 0.3:
            explanations.append("æƒ…ç»ªè¡¨è¾¾è¾ƒå¼º")

        if dimensions.get("political_relevance", 0) > 0.6:
            explanations.append("æ¶‰åŠæ•æ„Ÿæ”¿æ²»è¯é¢˜")
        elif dimensions.get("political_relevance", 0) > 0.3:
            explanations.append("æ¶‰åŠæ”¿æ²»ç›¸å…³å†…å®¹")

        # æ·»åŠ é£é™©ç­‰çº§è¯´æ˜
        if risk_level == "SEVERE":
            explanations.append("âš ï¸ ä¸¥é‡é£é™©ï¼šå†…å®¹å¯èƒ½è¿åå¹³å°æ”¿ç­–")
        elif risk_level == "HIGH":
            explanations.append("âš ï¸ é«˜é£é™©ï¼šå»ºè®®äººå·¥å®¡æ ¸")
        elif risk_level == "MODERATE":
            explanations.append("âš ï¸ ä¸­ç­‰é£é™©ï¼šéœ€è¦å…³æ³¨")

        if not explanations and risk_level == "LOW":
            explanations.append("âœ… å†…å®¹è¾ƒä¸ºä¸­æ€§ï¼Œæ— æ˜æ˜¾é£é™©")

        return explanations

    def _calculate_confidence(self, text: str, dimensions: Dict[str, float]) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦"""
        confidence = 0.7  # åŸºç¡€ç½®ä¿¡åº¦

        # æ–‡æœ¬é•¿åº¦å½±å“
        if len(text) > 50:
            confidence += 0.1
        elif len(text) < 10:
            confidence -= 0.2

        # ç»´åº¦åˆ†æ•°ä¸€è‡´æ€§å½±å“
        max_score = max(dimensions.values()) if dimensions else 0
        if max_score > 0.8:
            confidence += 0.1  # é«˜é£é™©å†…å®¹æ›´å®¹æ˜“åˆ¤æ–­
        elif max_score < 0.2:
            confidence += 0.05  # ä½é£é™©å†…å®¹ä¹Ÿç›¸å¯¹å®¹æ˜“åˆ¤æ–­

        return min(max(confidence, 0.5), 1.0)

    def _error_result(self, error_msg: str) -> Dict[str, Any]:
        """é”™è¯¯ç»“æœ"""
        return {
            "success": False,
            "error": error_msg,
            "risk_level": "UNKNOWN",
            "risk_score": 0.0,
            "dimensions": {
                "toxicity": 0.0,
                "hate_targeting": 0.0,
                "emotional_intensity": 0.0,
                "political_relevance": 0.0,
            },
            "explanations": [error_msg],
            "confidence": 0.0,
            "timestamp": datetime.now().isoformat(),
        }

    def batch_analyze(
        self, texts: List[str], language: str = "auto"
    ) -> List[Dict[str, Any]]:
        """æ‰¹é‡åˆ†ææ–‡æœ¬"""
        results = []
        for text in texts:
            results.append(self.analyze(text, language))
        return results


# ============================== å…¨å±€å®ä¾‹å’Œæ¥å£ ==============================

# åˆ›å»ºå…¨å±€åˆ†æå™¨å®ä¾‹
_global_analyzer = None


def get_analyzer() -> TiktokRiskAnalyzer:
    """è·å–åˆ†æå™¨å®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰"""
    global _global_analyzer
    if _global_analyzer is None:
        _global_analyzer = TiktokRiskAnalyzer()
    return _global_analyzer


def analyze_text(text: str, language: str = "auto") -> Dict[str, Any]:
    """
    åˆ†ææ–‡æœ¬é£é™© - ä¸»æ¥å£å‡½æ•°

    å‚æ•°:
        text: è¦åˆ†æçš„æ–‡æœ¬å†…å®¹
        language: æ–‡æœ¬è¯­è¨€ï¼ˆé»˜è®¤"auto"è‡ªåŠ¨æ£€æµ‹ï¼‰

    è¿”å›:
        åˆ†æç»“æœå­—å…¸
    """
    analyzer = get_analyzer()
    return analyzer.analyze(text, language)


def batch_analyze(texts: List[str], language: str = "auto") -> List[Dict[str, Any]]:
    """æ‰¹é‡åˆ†ææ–‡æœ¬"""
    analyzer = get_analyzer()
    return analyzer.batch_analyze(texts, language)


# ============================== å®‰è£…æ£€æŸ¥ ==============================


def check_dependencies():
    """æ£€æŸ¥ä¾èµ–åº“"""
    print("æ£€æŸ¥ä¾èµ–åº“...")

    required_packages = ["transformers", "langdetect"]

    missing_packages = []

    for package in required_packages:
        try:
            __import__(package.replace("-", "_"))
            print(f"âœ… {package}")
        except ImportError:
            print(f"âŒ {package} (éœ€è¦å®‰è£…)")
            missing_packages.append(package)

    if missing_packages:
        print(f"\nè¯·å®‰è£…ç¼ºå¤±çš„åŒ…: pip install {' '.join(missing_packages)}")
        return False

    print("\nâœ… æ‰€æœ‰ä¾èµ–åº“å·²å®‰è£…")
    return True


# ============================== æµ‹è¯•å’Œæ¼”ç¤º ==============================


def run_start():

    print("=" * 70)
    # å¤šè¯­è¨€æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {"lang": "auto", "text": "hello! ä½ å¥½! ã“ã‚“ã«ã¡ã¯!"},
        {"lang": "auto", "text": "ãƒã‚«ï¼ãŠå‰ãŒå¤§å«Œã„ã ï¼"},
        {"lang": "auto", "text": "ä½ ä¸ªäºŒè´§"},
        {
            "lang": "auto",
            "text": "You're such an IDIOT! I can't believe you did that!...",
        },
    ]

    for i, test in enumerate(test_cases, 1):
        print(f"\n{'='*50}")
        print(f"æµ‹è¯• {i}: [{test['lang'].upper()}] {test['text'][:60]}...")
        print(f"{'='*50}")

        result = analyze_text(test["text"], test["lang"])

        if result["success"]:
            print(f"âœ… æˆåŠŸ: {result['success']}")
            print(f"ğŸŒ æ£€æµ‹è¯­è¨€: {result['detected_language']}")
            print(f"âš ï¸  é£é™©ç­‰çº§: {result['risk_level']}")
            print(f"ğŸ“Š é£é™©åˆ†æ•°: {result['risk_score']}")
            print(f"â±ï¸  å¤„ç†æ—¶é—´: {result['processing_time']}ç§’")
            print(f"ğŸ“ˆ ç»´åº¦åˆ†æ:")
            for dim, score in result["dimensions"].items():
                bar = "â–ˆ" * int(score * 20)
                print(f"   {dim:20s} {score:.3f} {bar}")
            print(f"ğŸ’¬ è§£é‡Šè¯´æ˜:")
            for exp in result["explanations"]:
                print(f"   â€¢ {exp}")
        else:
            print(f"âŒ é”™è¯¯: {result['error']}")

    print(f"\n{'='*70}")


if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–
    if check_dependencies():
        # è¿è¡Œæ¼”ç¤º
        run_start()
